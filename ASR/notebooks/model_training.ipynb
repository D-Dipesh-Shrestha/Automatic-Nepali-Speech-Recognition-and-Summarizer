{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTI5sLw3WWBA",
        "outputId": "d0edddd5-eec8-42f3-a580-54c6c3f4f149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5LPD3JsZtmT",
        "outputId": "7f067138-4eb0-48dc-cd0d-bf8d2c978ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 14 09:55:32 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A63beGG_Zvox",
        "outputId": "165c96d0-4b89-4991-ba6e-25f212a3729b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "hLRrHseN61L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "device_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wjIiEXlV6xYd",
        "outputId": "bc404ab3-a292-4ef4-f2ff-939e4904a204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cloning Repo"
      ],
      "metadata": {
        "id": "UacoO9ysXZza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/manishdhakal/ASR-Nepali-using-CNN-BiLSTM-ResNet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TBqs5YQXT8c",
        "outputId": "cba9b974-62b9-47f6-ba26-7c93d84ddabc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ASR-Nepali-using-CNN-BiLSTM-ResNet'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 86 (delta 17), reused 80 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (86/86), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading Dataset"
      ],
      "metadata": {
        "id": "2G9xospPXbw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content\n",
        "# !mkdir ~/.kaggle \n",
        "# !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json \n",
        "# !chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vhf5d25CXXr3",
        "outputId": "15b9a41c-3498-4c3f-b85d-f4e05fa4e673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !kaggle datasets download anishshilpakar/asr-cnn-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYFlCOTmXtdp",
        "outputId": "0a003230-3f18-4a25-95d7-226da1925ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading asr-cnn-dataset.zip to /content\n",
            "100% 6.88G/6.88G [06:12<00:00, 23.4MB/s]\n",
            "100% 6.88G/6.88G [06:12<00:00, 19.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting"
      ],
      "metadata": {
        "id": "FqisW9jiX1h7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# %cd /content\n",
        "# !unzip asr-cnn-dataset"
      ],
      "metadata": {
        "id": "es24lKOGX1GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%cd /content\n",
        "!unzip /content/drive/MyDrive/audio.zip"
      ],
      "metadata": {
        "id": "XQFvtBej-5yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install edit_distance \n",
        "!pip install datasets\n",
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "D2YsZuccYUCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/ASR-Nepali-using-CNN-BiLSTM-ResNet/trainer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK3U_uI3Ems0",
        "outputId": "d4691bd3-989f-4c8b-95bb-99e936b26212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-14 11:12:26.865577: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "/content/drive/MyDrive/ASR-Nepali-using-CNN-BiLSTM-ResNet/trainer.py:19: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  wer_metric = load_metric(\"wer\")\n",
            "/device:GPU:0\n",
            "Model defined ✅ ✅ ✅ ✅\n",
            "\n",
            "Loading data.....\n",
            "There are 22695 files\n",
            "Data loaded ✅ ✅ ✅ ✅\n",
            "And It took 28.128363132476807 seconds\n",
            "\n",
            "Cleaning the audio files.....\n",
            "Audio files cleaned ✅ ✅ ✅ ✅\n",
            "And It took 18.67191505432129 seconds\n",
            "\n",
            "Generating mfcc features.....\n",
            "MFCC features generated ✅ ✅ ✅ ✅\n",
            "And It took 528.8424408435822 seconds\n",
            "\n",
            "Total Time for Loading Data: 575.6427190303802 seconds\n",
            "Training epoch: 1\n",
            "  9% 8/91 [05:50<52:48, 38.17s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/ASR-Nepali-using-CNN-BiLSTM-ResNet/trainer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fgu3VvuKwmF",
        "outputId": "47ff8436-e6a2-4214-8215-4565943df32b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-14 07:02:30.576175: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "/content/drive/MyDrive/ASR-Nepali-using-CNN-BiLSTM-ResNet/trainer.py:19: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  wer_metric = load_metric(\"wer\")\n",
            "/device:GPU:0\n",
            "Model defined ✅ ✅ ✅ ✅\n",
            "\n",
            "Loading data.....\n",
            "10000 data loaded !!!\n",
            "Data loaded ✅ ✅ ✅ ✅\n",
            "And It took 58.04688501358032 seconds\n",
            "\n",
            "Cleaning the audio files.....\n",
            "Audio files cleaned ✅ ✅ ✅ ✅\n",
            "And It took 19.06446361541748 seconds\n",
            "\n",
            "Generating mfcc features.....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/training_checkpoints"
      ],
      "metadata": {
        "id": "b3MN3DwcM5ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# with open('/content/drive/MyDrive/Automatic-Nepali-Speech-Recognition-and-Summarizer/ASR/data_asr/vocabulary_asr/new_vocab.json','r',encoding='utf8') as f:\n",
        "#     chars_dict = json.load(f)\n",
        "# unq_chars = list(chars_dict.keys())\n",
        "# unq_chars"
      ],
      "metadata": {
        "id": "NtwErL9vaErC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateWER(actual_label, predicted_label):\n",
        "    # convert string to list\n",
        "    actual_words = actual_label.split()\n",
        "    predicted_words = predicted_label.split()\n",
        "    # costs will hold the costs like in Levenshtein distance algorithm\n",
        "    costs = [[0 for inner in range(len(predicted_words)+1)] for outer in range(len(actual_words)+1)]\n",
        "    # backtrace will hold the operations we've done.\n",
        "    # so we could later backtrace, like the WER algorithm requires us to.\n",
        "    backtrace = [[0 for inner in range(len(predicted_words)+1)] for outer in range(len(actual_words)+1)]\n",
        "    # ok means no change, sub means substitution, ins means insertion and del means deletion\n",
        "    operations = {\n",
        "        'ok': 0,\n",
        "        'sub': 1,\n",
        "        'ins': 2,\n",
        "        'del': 3\n",
        "    }\n",
        "    # penalties for insertion, substitution and deletion\n",
        "    penalties = {\n",
        "        'ins': 1,\n",
        "        'sub': 1,\n",
        "        'del': 1\n",
        "    }\n",
        "    # First column represents the case where we achieve zero predicted labels i-e all the actual labels were deleted \n",
        "    for i in range(1,len(actual_words)+1):\n",
        "        costs[i][0] = penalties['del']*i \n",
        "        backtrace[i][0] = operations['del']\n",
        "    \n",
        "    # First row represents the case where we achieve the predicted label by inserting all the predicted labels into a zero length actual label i-e all unwanted insertions \n",
        "    for j in range(1,len(predicted_words)+1):\n",
        "        costs[0][j] = penalties['ins']*j \n",
        "        backtrace[0][j] = operations['ins']\n",
        "    \n",
        "    # computation\n",
        "    for i in  range(1,len(actual_words)+1):\n",
        "        for j in range(1,len(predicted_words)+1):\n",
        "            # no change in predictions and actual label\n",
        "            if actual_words[i-1] == predicted_words[j-1]:\n",
        "                costs[i][j] = costs[i-1][j-1]\n",
        "                backtrace[i][j] = operations['ok']\n",
        "            else:\n",
        "                # change has occured\n",
        "                sub_cost = costs[i-1][j-1] + penalties['sub']\n",
        "                ins_cost = costs[i][j-1] + penalties['ins']\n",
        "                del_cost = costs[i-1][j] + penalties['del']\n",
        "                costs[i][j] = min(sub_cost,ins_cost,del_cost)\n",
        "                if costs[i][j] == sub_cost:\n",
        "                    backtrace[i][j] = operations['sub']\n",
        "                elif costs[i][j] == ins_cost:\n",
        "                    backtrace[i][j] = operations['ins']\n",
        "                else: \n",
        "                    backtrace[i][j] = operations['del']\n",
        "    \n",
        "    # backtrace through the best route\n",
        "    i = len(actual_words)\n",
        "    j = len(predicted_words)\n",
        "    sub_count = 0 \n",
        "    del_count = 0 \n",
        "    ins_count = 0 \n",
        "    correct_count = 0 \n",
        "\n",
        "    while i > 0 or j > 0:\n",
        "        if backtrace[i][j] == operations['ok']:\n",
        "            correct_count += 1\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        elif backtrace[i][j] == operations['sub']:\n",
        "            sub_count += 1 \n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        elif backtrace[i][j] == operations['ins']:\n",
        "            ins_count += 1\n",
        "            j -= 1\n",
        "        elif backtrace[i][j] == operations['del']:\n",
        "            del_count += 1\n",
        "            i -= 1\n",
        "    \n",
        "    \"\"\" \n",
        "    WER formula: \n",
        "    WER = S + D + I / N = S + D I / S + D + C\n",
        "    \"\"\"\n",
        "    wer = round((sub_count + del_count + ins_count)/(sub_count + del_count + correct_count),3)\n",
        "    # wer = round((sub_count + ins_count + del_count)/(float)(len(actual_words)),3)\n",
        "    return wer "
      ],
      "metadata": {
        "id": "My_3_Zubh7NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the WER and CER \n",
        "def calculateErrorRates(actual_label,predicted_label):\n",
        "    # For CER\n",
        "    sm = ed.SequenceMatcher(predicted_label,actual_label)\n",
        "    ed_dist = sm.distance() \n",
        "    cer = ed_dist/len(actual_label)\n",
        "    # For WER \n",
        "    wer = calculateWER(actual_label,predicted_label)\n"
      ],
      "metadata": {
        "id": "V5wtCotp2kLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateErrorRates(actual_label,predicted_label):\n",
        "    # To load the evaluation metrics \n",
        "    wer_metric = load_metric(\"wer\")\n",
        "    cer_metric = load_metric(\"cer\",revision=\"master\")\n",
        "    # Calculate CER and WER for given arguments \n",
        "    cer = cer_metric.compute(predictions=[predicted_label],references=[actual_label])\n",
        "    wer = wer_metric.compute(predictions=[predicted_label],references=[actual_label])\n",
        "    return cer,wer"
      ],
      "metadata": {
        "id": "uIG8GR14iHAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateBatchErrorRates(output,target,start,end,cer,wer):\n",
        "    \"\"\"\n",
        "        The line of codes below is for computing evaluation metric (CER) on internal validation data.\n",
        "    \"\"\"\n",
        "    input_len = np.ones(output.shape[0]) * output.shape[1]\n",
        "    # Decode the output using beam search and CTC to get  the required logits\n",
        "    decoded_indices = K.ctc_decode(output, input_length=input_len,\n",
        "                            greedy=False, beam_width=100)[0][0]\n",
        "    \n",
        "    # Remove the padding token from batchified target texts\n",
        "    target_indices = [sent[sent != 0].tolist() for sent in target]\n",
        "\n",
        "    # Remove the padding, unknown token, and blank token from predicted texts\n",
        "    predicted_indices = [sent[sent > 1].numpy().tolist() for sent in decoded_indices] # idx 0: padding token, idx 1: unknown, idx -1: blank token\n",
        "\n",
        "    batch_cer = cer\n",
        "    batch_wer = wer\n",
        "    len_batch = end - start\n",
        "    for i in range(len_batch):\n",
        "        predicted_label = predicted_indices[i]\n",
        "        actual_label = target_indices[i]\n",
        "        error_rates = calculateErrorRates(actual_label,predicted_label)\n",
        "        batch_cer += error_rates[0]\n",
        "        batch_wer += error_rates[1]\n",
        "    batch_cer /= len_batch\n",
        "    batch_wer /= len_batch\n",
        "    return batch_cer,batch_wer"
      ],
      "metadata": {
        "id": "rNhYVbfek9px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, train_wavs, train_texts, validation_wavs, validation_texts, epochs=100, batch_size=50):\n",
        "\n",
        "    with tf.device(device_name):\n",
        "        # These will be the final results to be returned\n",
        "        train_losses = []\n",
        "        validation_losses = []\n",
        "        train_CERs = []\n",
        "        train_WERs = []\n",
        "        validation_CERs = []\n",
        "        validation_WERs = []\n",
        "        for e in range(0, epochs):\n",
        "            start_time = time.time()\n",
        "            len_train = len(train_wavs)\n",
        "            len_validation = len(validation_wavs)\n",
        "            train_loss = 0\n",
        "            training_CER = 0\n",
        "            training_WER = 0\n",
        "            validation_loss = 0\n",
        "            validation_CER = 0\n",
        "            validation_WER = 0\n",
        "            train_batch_count = 0\n",
        "            validation_batch_count = 0\n",
        "            # Training Steps\n",
        "            print(\"Training epoch: {}\".format(e+1))\n",
        "            for start in tqdm(range(0, len_train, batch_size)):\n",
        "\n",
        "                end = None\n",
        "                if start + batch_size < len_train:\n",
        "                    end = start + batch_size\n",
        "                else:\n",
        "                    end = len_train\n",
        "                x, target, target_lengths, output_lengths = batchify(\n",
        "                    train_wavs[start:end], train_texts[start:end], UNQ_CHARS)\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    output = model(x, training=True)\n",
        "\n",
        "                    loss = K.ctc_batch_cost(\n",
        "                        target, output, output_lengths, target_lengths)\n",
        "\n",
        "                grads = tape.gradient(loss, model.trainable_weights)\n",
        "                optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "                train_loss += np.average(loss.numpy())\n",
        "                train_batch_count += 1\n",
        "                training_cer, training_wer = calculateBatchErrorRates(output,target,start,end,training_CER,training_WER)\n",
        "\n",
        "\n",
        "            # Validation Step\n",
        "            print(\"Validation epoch: {}\".format(e+1))\n",
        "            for start in tqdm(range(0, len_validation, batch_size)):\n",
        "\n",
        "                end = None\n",
        "                if start + batch_size < len_validation:\n",
        "                    end = start + batch_size\n",
        "                else:\n",
        "                    end = len_validation\n",
        "                x, target, target_lengths, output_lengths = batchify(\n",
        "                    validation_wavs[start:end], validation_texts[start:end], UNQ_CHARS)\n",
        "\n",
        "                output = model(x, training=False)\n",
        "\n",
        "                # Calculate CTC Loss\n",
        "                loss = K.ctc_batch_cost(\n",
        "                    target, output, output_lengths, target_lengths)\n",
        "\n",
        "                validation_loss += np.average(loss.numpy())\n",
        "                validation_batch_count += 1\n",
        "                validation_cer, validation_wer = calculateBatchErrorRates(output,target,start,end,validation_CER,validation_WER)\n",
        "\n",
        "            # Average the results\n",
        "            # losses\n",
        "            train_loss /= train_batch_count\n",
        "            validation_loss /= validation_batch_count\n",
        "            # cers\n",
        "            train_CER /= train_batch_count\n",
        "            validation_CER /= validation_batch_count\n",
        "            # wers \n",
        "            train_WER /= train_batch_count \n",
        "            validation_WER /= validation_batch_count\n",
        "\n",
        "            # Append the results \n",
        "            train_losses.append(train_loss)\n",
        "            train_CERs.append(train_CER)\n",
        "            train_WERs.append(train_WER)\n",
        "            validation_losses.append(validation_loss)\n",
        "            validation_CERs.append(validation_CER)\n",
        "            validation_WERs.append(validation_WER)\n",
        "            \n",
        "            rec = f\"Epoch: {e+1}, Train Loss: {train_loss:.2f}, Validation Loss: {validation_loss:.2f}, Train CER: {(train_CER*100):.2f}, Validation CER: {(validation_CER*100):.2f}, Train WER: {(train_WER*100):.2f}, Validation WER: {(validation_WER*100):.2f} in {(time.time() - start_time):.2f} secs\\n\"\n",
        "\n",
        "            print(rec)\n",
        "\n",
        "        result = {\n",
        "            'epochs': range(0,epochs),\n",
        "            'train_loss': train_losses,\n",
        "            'validation_loss': validation_losses,\n",
        "            'train_cer': train_CERs,\n",
        "            'validation_cer': validation_CERs,\n",
        "            'train_wer': train_WERs,\n",
        "            'validation_wer': validation_WERs\n",
        "        }\n",
        "    \n",
        "    return model, result"
      ],
      "metadata": {
        "id": "OAndIGiOhHUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_model(model, optimizer, train_wavs, train_texts, test_wavs, test_texts, epochs=100, batch_size=50):\n",
        "\n",
        "#     with tf.device(device_name):\n",
        "#         train_losses = []\n",
        "#         validation_losses = []\n",
        "#         train_CERs = []\n",
        "#         train_WERs = []\n",
        "#         validation_CERs = []\n",
        "#         validation_WERs = []\n",
        "#         for e in range(0, epochs):\n",
        "#             start_time = time.time()\n",
        "\n",
        "#             len_train = len(train_wavs)\n",
        "#             len_test = len(test_wavs)\n",
        "#             train_loss = 0\n",
        "#             test_loss = 0\n",
        "#             test_CER = 0\n",
        "#             train_batch_count = 0\n",
        "#             test_batch_count = 0\n",
        "\n",
        "#             print(\"Training epoch: {}\".format(e+1))\n",
        "#             for start in tqdm(range(0, len_train, batch_size)):\n",
        "\n",
        "#                 end = None\n",
        "#                 if start + batch_size < len_train:\n",
        "#                     end = start + batch_size\n",
        "#                 else:\n",
        "#                     end = len_train\n",
        "#                 x, target, target_lengths, output_lengths = batchify(\n",
        "#                     train_wavs[start:end], train_texts[start:end], UNQ_CHARS)\n",
        "\n",
        "#                 with tf.GradientTape() as tape:\n",
        "#                     output = model(x, training=True)\n",
        "\n",
        "#                     loss = K.ctc_batch_cost(\n",
        "#                         target, output, output_lengths, target_lengths)\n",
        "\n",
        "#                 grads = tape.gradient(loss, model.trainable_weights)\n",
        "#                 optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "#                 train_loss += np.average(loss.numpy())\n",
        "#                 train_batch_count += 1\n",
        "\n",
        "#             print(\"Testing epoch: {}\".format(e+1))\n",
        "#             for start in tqdm(range(0, len_test, batch_size)):\n",
        "\n",
        "#                 end = None\n",
        "#                 if start + batch_size < len_test:\n",
        "#                     end = start + batch_size\n",
        "#                 else:\n",
        "#                     end = len_test\n",
        "#                 x, target, target_lengths, output_lengths = batchify(\n",
        "#                     test_wavs[start:end], test_texts[start:end], UNQ_CHARS)\n",
        "\n",
        "#                 output = model(x, training=False)\n",
        "\n",
        "#                 # Calculate CTC Loss\n",
        "#                 loss = K.ctc_batch_cost(\n",
        "#                     target, output, output_lengths, target_lengths)\n",
        "\n",
        "#                 test_loss += np.average(loss.numpy())\n",
        "#                 test_batch_count += 1\n",
        "\n",
        "#                 \"\"\"\n",
        "#                     The line of codes below is for computing evaluation metric (CER) on internal validation data.\n",
        "#                 \"\"\"\n",
        "#                 input_len = np.ones(output.shape[0]) * output.shape[1]\n",
        "#                 decoded_indices = K.ctc_decode(output, input_length=input_len,\n",
        "#                                        greedy=False, beam_width=100)[0][0]\n",
        "                \n",
        "#                 # Remove the padding token from batchified target texts\n",
        "#                 target_indices = [sent[sent != 0].tolist() for sent in target]\n",
        "\n",
        "#                 # Remove the padding, unknown token, and blank token from predicted texts\n",
        "#                 predicted_indices = [sent[sent > 1].numpy().tolist() for sent in decoded_indices] # idx 0: padding token, idx 1: unknown, idx -1: blank token\n",
        "\n",
        "#                 len_batch = end - start\n",
        "#                 for i in range(len_batch):\n",
        "\n",
        "#                     pred = predicted_indices[i]\n",
        "#                     truth = target_indices[i]\n",
        "#                     sm = ed.SequenceMatcher(pred, truth)\n",
        "#                     ed_dist = sm.distance()                 # Edit distance\n",
        "#                     test_CER += ed_dist / len(truth)\n",
        "#                 test_CER /= len_batch\n",
        "\n",
        "#             train_loss /= train_batch_count\n",
        "#             test_loss /= test_batch_count\n",
        "#             test_CER /= test_batch_count\n",
        "\n",
        "#             rec = \"Epoch: {}, Train Loss: {:.2f}, Test Loss {:.2f}, Test CER {:.2f} % in {:.2f} secs.\\n\".format(\n",
        "#                 e+1, train_loss, test_loss, test_CER*100, time.time() - start_time)\n",
        "\n",
        "#             print(rec)\n",
        "#         return model"
      ],
      "metadata": {
        "id": "b5H79KizE-jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "len(os.listdir('/content/audio'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVZJLy2ZBVAq",
        "outputId": "e36b1ddb-3892-4172-d512-b32f46f1b891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "142695"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/ASR-Nepali-using-CNN-BiLSTM-ResNet/trainer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-1qD2G9Yg_L",
        "outputId": "1a0b5049-ad3e-4137-9767-80796cb9ac10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-13 16:59:00.181764: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "/content/drive/MyDrive/ASR-Nepali-using-CNN-BiLSTM-ResNet/trainer.py:18: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  wer_metric = load_metric(\"wer\")\n",
            "Model defined ✅ ✅ ✅ ✅\n",
            "\n",
            "Loading data.....\n",
            "Data loaded ✅ ✅ ✅ ✅\n",
            "\n",
            "Cleaning the audio files.....\n",
            "Audio files cleaned ✅ ✅ ✅ ✅\n",
            "\n",
            "Generating mfcc features.....\n",
            "MFCC features generated ✅ ✅ ✅ ✅\n",
            "\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Training epoch: 1\n",
            "100% 4/4 [00:10<00:00,  2.61s/it]\n",
            "Validation epoch: 1\n",
            "100% 1/1 [00:01<00:00,  1.31s/it]\n",
            "Epoch: 1, Train Loss: 1280.79, Validation Loss: 1693.57, Train CER: 25.54, Validation CER: 82.85, Train WER: 41.78, Validation WER: 93.38 in 11.76 secs\n",
            "\n",
            "Training epoch: 2\n",
            "100% 4/4 [00:06<00:00,  1.56s/it]\n",
            "Validation epoch: 2\n",
            "100% 1/1 [00:01<00:00,  1.21s/it]\n",
            "Epoch: 2, Train Loss: 1011.85, Validation Loss: 749.94, Train CER: 39.82, Validation CER: 99.43, Train WER: 46.56, Validation WER: 100.00 in 7.43 secs\n",
            "\n",
            "Training epoch: 3\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 3\n",
            "100% 1/1 [00:01<00:00,  1.21s/it]\n",
            "Epoch: 3, Train Loss: 752.52, Validation Loss: 703.05, Train CER: 33.76, Validation CER: 99.43, Train WER: 40.59, Validation WER: 100.00 in 7.11 secs\n",
            "\n",
            "Training epoch: 4\n",
            "100% 4/4 [00:06<00:00,  1.58s/it]\n",
            "Validation epoch: 4\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 4, Train Loss: 790.66, Validation Loss: 752.43, Train CER: 29.48, Validation CER: 99.43, Train WER: 48.76, Validation WER: 100.00 in 7.54 secs\n",
            "\n",
            "Training epoch: 5\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 5\n",
            "100% 1/1 [00:01<00:00,  1.26s/it]\n",
            "Epoch: 5, Train Loss: 611.73, Validation Loss: 847.54, Train CER: 35.95, Validation CER: 99.62, Train WER: 42.20, Validation WER: 100.00 in 7.16 secs\n",
            "\n",
            "Training epoch: 6\n",
            "100% 4/4 [00:05<00:00,  1.45s/it]\n",
            "Validation epoch: 6\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 6, Train Loss: 603.56, Validation Loss: 728.63, Train CER: 36.95, Validation CER: 99.33, Train WER: 40.29, Validation WER: 100.00 in 7.00 secs\n",
            "\n",
            "Training epoch: 7\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 7\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 7, Train Loss: 566.74, Validation Loss: 616.69, Train CER: 35.16, Validation CER: 99.33, Train WER: 39.74, Validation WER: 100.00 in 7.10 secs\n",
            "\n",
            "Training epoch: 8\n",
            "100% 4/4 [00:06<00:00,  1.50s/it]\n",
            "Validation epoch: 8\n",
            "100% 1/1 [00:01<00:00,  1.25s/it]\n",
            "Epoch: 8, Train Loss: 628.30, Validation Loss: 554.44, Train CER: 33.27, Validation CER: 99.05, Train WER: 39.26, Validation WER: 100.00 in 7.26 secs\n",
            "\n",
            "Training epoch: 9\n",
            "100% 4/4 [00:07<00:00,  1.92s/it]\n",
            "Validation epoch: 9\n",
            "100% 1/1 [00:01<00:00,  1.29s/it]\n",
            "Epoch: 9, Train Loss: 557.66, Validation Loss: 491.23, Train CER: 34.49, Validation CER: 99.43, Train WER: 39.72, Validation WER: 100.00 in 8.99 secs\n",
            "\n",
            "Training epoch: 10\n",
            "100% 4/4 [00:06<00:00,  1.52s/it]\n",
            "Validation epoch: 10\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 10, Train Loss: 536.00, Validation Loss: 475.31, Train CER: 36.02, Validation CER: 99.43, Train WER: 40.00, Validation WER: 100.00 in 7.31 secs\n",
            "\n",
            "Training epoch: 11\n",
            "100% 4/4 [00:05<00:00,  1.45s/it]\n",
            "Validation epoch: 11\n",
            "100% 1/1 [00:01<00:00,  1.26s/it]\n",
            "Epoch: 11, Train Loss: 519.79, Validation Loss: 515.63, Train CER: 38.59, Validation CER: 99.43, Train WER: 40.73, Validation WER: 100.00 in 7.07 secs\n",
            "\n",
            "Training epoch: 12\n",
            "100% 4/4 [00:05<00:00,  1.45s/it]\n",
            "Validation epoch: 12\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 12, Train Loss: 510.44, Validation Loss: 519.98, Train CER: 38.74, Validation CER: 99.05, Train WER: 41.12, Validation WER: 100.00 in 7.03 secs\n",
            "\n",
            "Training epoch: 13\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 13\n",
            "100% 1/1 [00:01<00:00,  1.24s/it]\n",
            "Epoch: 13, Train Loss: 501.77, Validation Loss: 528.24, Train CER: 36.68, Validation CER: 99.05, Train WER: 39.97, Validation WER: 100.00 in 7.14 secs\n",
            "\n",
            "Training epoch: 14\n",
            "100% 4/4 [00:06<00:00,  1.50s/it]\n",
            "Validation epoch: 14\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 14, Train Loss: 494.66, Validation Loss: 548.36, Train CER: 37.03, Validation CER: 99.43, Train WER: 40.01, Validation WER: 100.00 in 7.24 secs\n",
            "\n",
            "Training epoch: 15\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 15\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 15, Train Loss: 489.09, Validation Loss: 532.12, Train CER: 36.59, Validation CER: 99.43, Train WER: 40.20, Validation WER: 100.00 in 7.14 secs\n",
            "\n",
            "Training epoch: 16\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 16\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 16, Train Loss: 484.91, Validation Loss: 520.51, Train CER: 36.38, Validation CER: 99.43, Train WER: 40.15, Validation WER: 100.00 in 7.07 secs\n",
            "\n",
            "Training epoch: 17\n",
            "100% 4/4 [00:07<00:00,  1.88s/it]\n",
            "Validation epoch: 17\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 17, Train Loss: 481.24, Validation Loss: 510.46, Train CER: 36.51, Validation CER: 98.00, Train WER: 39.79, Validation WER: 99.26 in 8.73 secs\n",
            "\n",
            "Training epoch: 18\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 18\n",
            "100% 1/1 [00:01<00:00,  1.24s/it]\n",
            "Epoch: 18, Train Loss: 477.13, Validation Loss: 492.65, Train CER: 36.41, Validation CER: 97.34, Train WER: 39.72, Validation WER: 98.53 in 7.13 secs\n",
            "\n",
            "Training epoch: 19\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 19\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 19, Train Loss: 474.36, Validation Loss: 502.72, Train CER: 36.44, Validation CER: 96.56, Train WER: 39.43, Validation WER: 97.79 in 7.08 secs\n",
            "\n",
            "Training epoch: 20\n",
            "100% 4/4 [00:05<00:00,  1.45s/it]\n",
            "Validation epoch: 20\n",
            "100% 1/1 [00:01<00:00,  1.25s/it]\n",
            "Epoch: 20, Train Loss: 471.21, Validation Loss: 543.75, Train CER: 36.43, Validation CER: 96.85, Train WER: 39.27, Validation WER: 98.16 in 7.06 secs\n",
            "\n",
            "Training epoch: 21\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 21\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 21, Train Loss: 468.74, Validation Loss: 571.98, Train CER: 36.11, Validation CER: 94.08, Train WER: 39.14, Validation WER: 95.59 in 7.09 secs\n",
            "\n",
            "Training epoch: 22\n",
            "100% 4/4 [00:05<00:00,  1.49s/it]\n",
            "Validation epoch: 22\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 22, Train Loss: 466.38, Validation Loss: 602.82, Train CER: 36.33, Validation CER: 95.12, Train WER: 39.56, Validation WER: 96.32 in 7.17 secs\n",
            "\n",
            "Training epoch: 23\n",
            "100% 4/4 [00:05<00:00,  1.45s/it]\n",
            "Validation epoch: 23\n",
            "100% 1/1 [00:01<00:00,  1.26s/it]\n",
            "Epoch: 23, Train Loss: 463.38, Validation Loss: 589.95, Train CER: 36.17, Validation CER: 94.45, Train WER: 39.30, Validation WER: 95.59 in 7.07 secs\n",
            "\n",
            "Training epoch: 24\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 24\n",
            "100% 1/1 [00:01<00:00,  1.25s/it]\n",
            "Epoch: 24, Train Loss: 460.56, Validation Loss: 605.70, Train CER: 36.21, Validation CER: 93.01, Train WER: 39.12, Validation WER: 94.85 in 7.10 secs\n",
            "\n",
            "Training epoch: 25\n",
            "100% 4/4 [00:07<00:00,  1.90s/it]\n",
            "Validation epoch: 25\n",
            "100% 1/1 [00:01<00:00,  1.26s/it]\n",
            "Epoch: 25, Train Loss: 459.02, Validation Loss: 643.99, Train CER: 35.79, Validation CER: 92.44, Train WER: 38.76, Validation WER: 94.12 in 8.87 secs\n",
            "\n",
            "Training epoch: 26\n",
            "100% 4/4 [00:06<00:00,  1.50s/it]\n",
            "Validation epoch: 26\n",
            "100% 1/1 [00:01<00:00,  1.28s/it]\n",
            "Epoch: 26, Train Loss: 456.72, Validation Loss: 786.98, Train CER: 36.02, Validation CER: 93.20, Train WER: 39.17, Validation WER: 94.12 in 7.29 secs\n",
            "\n",
            "Training epoch: 27\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 27\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 27, Train Loss: 453.91, Validation Loss: 787.60, Train CER: 36.03, Validation CER: 92.53, Train WER: 39.05, Validation WER: 93.75 in 7.13 secs\n",
            "\n",
            "Training epoch: 28\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 28\n",
            "100% 1/1 [00:01<00:00,  1.21s/it]\n",
            "Epoch: 28, Train Loss: 451.39, Validation Loss: 687.75, Train CER: 35.63, Validation CER: 92.15, Train WER: 38.52, Validation WER: 93.75 in 7.04 secs\n",
            "\n",
            "Training epoch: 29\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 29\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 29, Train Loss: 449.32, Validation Loss: 637.04, Train CER: 35.79, Validation CER: 92.15, Train WER: 38.65, Validation WER: 93.75 in 7.10 secs\n",
            "\n",
            "Training epoch: 30\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 30\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 30, Train Loss: 446.64, Validation Loss: 593.74, Train CER: 36.32, Validation CER: 91.19, Train WER: 39.27, Validation WER: 93.01 in 7.13 secs\n",
            "\n",
            "Training epoch: 31\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 31\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 31, Train Loss: 443.89, Validation Loss: 590.29, Train CER: 35.69, Validation CER: 91.10, Train WER: 38.86, Validation WER: 93.01 in 7.09 secs\n",
            "\n",
            "Training epoch: 32\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 32\n",
            "100% 1/1 [00:01<00:00,  1.21s/it]\n",
            "Epoch: 32, Train Loss: 441.90, Validation Loss: 579.92, Train CER: 35.89, Validation CER: 90.52, Train WER: 38.75, Validation WER: 92.65 in 7.07 secs\n",
            "\n",
            "Training epoch: 33\n",
            "100% 4/4 [00:07<00:00,  1.88s/it]\n",
            "Validation epoch: 33\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 33, Train Loss: 445.13, Validation Loss: 577.09, Train CER: 35.25, Validation CER: 89.18, Train WER: 38.28, Validation WER: 91.91 in 8.77 secs\n",
            "\n",
            "Training epoch: 34\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 34\n",
            "100% 1/1 [00:01<00:00,  1.25s/it]\n",
            "Epoch: 34, Train Loss: 439.25, Validation Loss: 666.87, Train CER: 35.26, Validation CER: 88.22, Train WER: 38.74, Validation WER: 91.91 in 7.15 secs\n",
            "\n",
            "Training epoch: 35\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 35\n",
            "100% 1/1 [00:01<00:00,  1.26s/it]\n",
            "Epoch: 35, Train Loss: 439.19, Validation Loss: 724.58, Train CER: 35.29, Validation CER: 88.70, Train WER: 38.48, Validation WER: 91.54 in 7.19 secs\n",
            "\n",
            "Training epoch: 36\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 36\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 36, Train Loss: 435.62, Validation Loss: 721.14, Train CER: 35.54, Validation CER: 87.36, Train WER: 38.26, Validation WER: 91.18 in 7.11 secs\n",
            "\n",
            "Training epoch: 37\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 37\n",
            "100% 1/1 [00:01<00:00,  1.20s/it]\n",
            "Epoch: 37, Train Loss: 431.94, Validation Loss: 707.94, Train CER: 34.69, Validation CER: 87.07, Train WER: 38.19, Validation WER: 91.54 in 7.06 secs\n",
            "\n",
            "Training epoch: 38\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 38\n",
            "100% 1/1 [00:01<00:00,  1.26s/it]\n",
            "Epoch: 38, Train Loss: 431.12, Validation Loss: 747.13, Train CER: 35.51, Validation CER: 87.74, Train WER: 38.56, Validation WER: 91.54 in 7.08 secs\n",
            "\n",
            "Training epoch: 39\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 39\n",
            "100% 1/1 [00:01<00:00,  1.21s/it]\n",
            "Epoch: 39, Train Loss: 427.91, Validation Loss: 769.90, Train CER: 34.64, Validation CER: 87.07, Train WER: 37.99, Validation WER: 91.54 in 7.05 secs\n",
            "\n",
            "Training epoch: 40\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 40\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 40, Train Loss: 424.92, Validation Loss: 781.29, Train CER: 34.85, Validation CER: 86.31, Train WER: 38.23, Validation WER: 91.54 in 7.14 secs\n",
            "\n",
            "Training epoch: 41\n",
            "100% 4/4 [00:07<00:00,  1.88s/it]\n",
            "Validation epoch: 41\n",
            "100% 1/1 [00:01<00:00,  1.27s/it]\n",
            "Epoch: 41, Train Loss: 422.30, Validation Loss: 783.61, Train CER: 35.09, Validation CER: 86.79, Train WER: 38.14, Validation WER: 91.54 in 8.78 secs\n",
            "\n",
            "Training epoch: 42\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 42\n",
            "100% 1/1 [00:01<00:00,  1.25s/it]\n",
            "Epoch: 42, Train Loss: 418.24, Validation Loss: 775.93, Train CER: 34.98, Validation CER: 86.50, Train WER: 38.15, Validation WER: 91.18 in 7.13 secs\n",
            "\n",
            "Training epoch: 43\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 43\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 43, Train Loss: 416.24, Validation Loss: 780.63, Train CER: 34.68, Validation CER: 85.06, Train WER: 37.74, Validation WER: 90.44 in 7.09 secs\n",
            "\n",
            "Training epoch: 44\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 44\n",
            "100% 1/1 [00:01<00:00,  1.19s/it]\n",
            "Epoch: 44, Train Loss: 413.20, Validation Loss: 805.22, Train CER: 34.02, Validation CER: 86.79, Train WER: 37.39, Validation WER: 91.54 in 7.09 secs\n",
            "\n",
            "Training epoch: 45\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 45\n",
            "100% 1/1 [00:01<00:00,  1.21s/it]\n",
            "Epoch: 45, Train Loss: 410.66, Validation Loss: 802.48, Train CER: 34.88, Validation CER: 86.12, Train WER: 38.13, Validation WER: 91.91 in 7.06 secs\n",
            "\n",
            "Training epoch: 46\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 46\n",
            "100% 1/1 [00:01<00:00,  1.24s/it]\n",
            "Epoch: 46, Train Loss: 408.34, Validation Loss: 798.20, Train CER: 34.65, Validation CER: 86.88, Train WER: 37.68, Validation WER: 91.91 in 7.16 secs\n",
            "\n",
            "Training epoch: 47\n",
            "100% 4/4 [00:05<00:00,  1.49s/it]\n",
            "Validation epoch: 47\n",
            "100% 1/1 [00:01<00:00,  1.24s/it]\n",
            "Epoch: 47, Train Loss: 407.72, Validation Loss: 814.06, Train CER: 33.90, Validation CER: 87.17, Train WER: 36.71, Validation WER: 91.54 in 7.18 secs\n",
            "\n",
            "Training epoch: 48\n",
            "100% 4/4 [00:06<00:00,  1.50s/it]\n",
            "Validation epoch: 48\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 48, Train Loss: 402.29, Validation Loss: 821.85, Train CER: 34.28, Validation CER: 85.83, Train WER: 37.16, Validation WER: 90.81 in 7.23 secs\n",
            "\n",
            "Training epoch: 49\n",
            "100% 4/4 [00:06<00:00,  1.73s/it]\n",
            "Validation epoch: 49\n",
            "100% 1/1 [00:01<00:00,  1.91s/it]\n",
            "Epoch: 49, Train Loss: 400.22, Validation Loss: 822.76, Train CER: 35.11, Validation CER: 87.74, Train WER: 37.69, Validation WER: 92.65 in 8.81 secs\n",
            "\n",
            "Training epoch: 50\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 50\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 50, Train Loss: 396.92, Validation Loss: 829.29, Train CER: 33.67, Validation CER: 84.58, Train WER: 37.21, Validation WER: 92.28 in 7.13 secs\n",
            "\n",
            "Training epoch: 51\n",
            "100% 4/4 [00:05<00:00,  1.49s/it]\n",
            "Validation epoch: 51\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 51, Train Loss: 392.96, Validation Loss: 847.86, Train CER: 33.25, Validation CER: 85.83, Train WER: 36.41, Validation WER: 90.44 in 7.18 secs\n",
            "\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Checkpoint Saved\n",
            "Training epoch: 52\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 52\n",
            "100% 1/1 [00:01<00:00,  1.24s/it]\n",
            "Epoch: 52, Train Loss: 390.42, Validation Loss: 860.62, Train CER: 34.98, Validation CER: 86.02, Train WER: 37.45, Validation WER: 91.54 in 7.15 secs\n",
            "\n",
            "Training epoch: 53\n",
            "100% 4/4 [00:05<00:00,  1.45s/it]\n",
            "Validation epoch: 53\n",
            "100% 1/1 [00:01<00:00,  1.26s/it]\n",
            "Epoch: 53, Train Loss: 386.09, Validation Loss: 854.74, Train CER: 33.60, Validation CER: 84.68, Train WER: 36.27, Validation WER: 90.07 in 7.06 secs\n",
            "\n",
            "Training epoch: 54\n",
            "100% 4/4 [00:06<00:00,  1.61s/it]\n",
            "Validation epoch: 54\n",
            "100% 1/1 [00:01<00:00,  1.33s/it]\n",
            "Epoch: 54, Train Loss: 384.71, Validation Loss: 843.70, Train CER: 33.03, Validation CER: 83.15, Train WER: 36.62, Validation WER: 90.81 in 7.76 secs\n",
            "\n",
            "Training epoch: 55\n",
            "100% 4/4 [00:06<00:00,  1.73s/it]\n",
            "Validation epoch: 55\n",
            "100% 1/1 [00:01<00:00,  1.80s/it]\n",
            "Epoch: 55, Train Loss: 380.96, Validation Loss: 856.30, Train CER: 34.68, Validation CER: 83.24, Train WER: 37.16, Validation WER: 91.18 in 8.71 secs\n",
            "\n",
            "Training epoch: 56\n",
            "100% 4/4 [00:06<00:00,  1.66s/it]\n",
            "Validation epoch: 56\n",
            "100% 1/1 [00:01<00:00,  1.24s/it]\n",
            "Epoch: 56, Train Loss: 375.76, Validation Loss: 869.86, Train CER: 33.26, Validation CER: 83.82, Train WER: 36.13, Validation WER: 90.81 in 7.86 secs\n",
            "\n",
            "Training epoch: 57\n",
            "100% 4/4 [00:07<00:00,  1.95s/it]\n",
            "Validation epoch: 57\n",
            "100% 1/1 [00:01<00:00,  1.24s/it]\n",
            "Epoch: 57, Train Loss: 372.25, Validation Loss: 857.54, Train CER: 31.78, Validation CER: 82.00, Train WER: 35.32, Validation WER: 90.07 in 9.04 secs\n",
            "\n",
            "Training epoch: 58\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 58\n",
            "100% 1/1 [00:01<00:00,  1.28s/it]\n",
            "Epoch: 58, Train Loss: 370.01, Validation Loss: 830.14, Train CER: 33.89, Validation CER: 83.34, Train WER: 36.51, Validation WER: 91.18 in 7.21 secs\n",
            "\n",
            "Training epoch: 59\n",
            "100% 4/4 [00:07<00:00,  1.92s/it]\n",
            "Validation epoch: 59\n",
            "100% 1/1 [00:02<00:00,  2.02s/it]\n",
            "Epoch: 59, Train Loss: 368.12, Validation Loss: 862.22, Train CER: 32.59, Validation CER: 84.01, Train WER: 35.72, Validation WER: 92.28 in 9.69 secs\n",
            "\n",
            "Training epoch: 60\n",
            "100% 4/4 [00:09<00:00,  2.29s/it]\n",
            "Validation epoch: 60\n",
            "100% 1/1 [00:01<00:00,  1.99s/it]\n",
            "Epoch: 60, Train Loss: 366.66, Validation Loss: 890.17, Train CER: 33.07, Validation CER: 82.48, Train WER: 36.10, Validation WER: 90.07 in 11.15 secs\n",
            "\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Model Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import pandas as pd "
      ],
      "metadata": {
        "id": "XwnxyMvMcYub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_df = pd.read_csv('/content/drive/MyDrive/Automatic-Nepali-Speech-Recognition-and-Summarizer/ASR/data_asr/transcript_asr/new_transcript_for_asr_complete.csv')[:10]\n",
        "train_wavs = []\n",
        "for f_name in texts_df[\"filename\"]:\n",
        "    wav, _ = librosa.load(f\"/content/audio/audio/{f_name}.flac\", sr=16000)\n",
        "    train_wavs.append(wav)\n",
        "train_texts = texts_df[\"label\"].tolist()\n",
        "print(train_wavs[-1],train_texts[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maCdGG2nZQoq",
        "outputId": "28c6da3c-bea0-4164-e1a8-13bd679fbe61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.9156721e-05  1.8241990e-05  3.8159305e-06 ... -8.6012187e-07\n",
            "  7.8891588e-07 -6.7354171e-07] आन्तरिक राजस्व कार्यालय भैरहवा अन्तर्गत रुपन्देहीको सिद्धार्थनगर नगरपालिका लुम्बिनी नगरपालिका तिलोत्तमा नगरपालिकामा मूल्य अभिवृद्धि करमा दर्ता छ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/ASR-Nepali-using-CNN-BiLSTM-ResNet/eval.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZhEyaeZckly",
        "outputId": "73843f2e-097a-4b57-9485-d224d8e7cfee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-11 16:33:13.369498: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Loading model.....\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Model loaded ✅ ✅ ✅ ✅\n",
            "\n",
            "Loading wav files.....\n",
            "Wav files loaded ✅ ✅ ✅ ✅\n",
            "\n",
            "Predicting sentences.....\n",
            "['र्ोरनापछि चलचित्रको कथाको', 'जो इ स्लामको पहिलो'] \n",
            "\n",
            "Calculating CER.....\n",
            "CER -> 13.29%, \t No.of sentences -> 2, \t Time Taken -> 2.15 secs.\n",
            "The total time taken for all sentences CER calculation is  2.15 secs.\n",
            "0.1328976034858388 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZMsyZ5SiKNJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}